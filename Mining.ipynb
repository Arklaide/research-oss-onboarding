{"cells":[{"cell_type":"markdown","metadata":{},"source":["You need to set up your own notebook/python enviroment in order to run this. We do not include a guide on the versions used. However, pydriller==1.15.5 was used, newer versions used 'Repository' instead of 'RepositoryMining'."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["github_token = 'YOUR_GITHUB_TOKEN'\n","github_username = 'YOUR_GITHUB_USERNAME'"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"09c51f5be5d44ab7af620c1468d7d5d1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":40,"execution_start":1669824114374,"source_hash":"6b08fbe2","tags":[]},"outputs":[],"source":["\n","from git import Repo\n","import os\n","import shutil\n","\n","def cloneRepo(repo_name):\n","    path = repo_name.split(\"/\")[1]\n","    Repo.clone_from(\"https://github.com/{rn}.git\".format(rn=repo_name), path)\n","\n","def deleteRepo(repo_name):\n","    path = './' + repo_name.split(\"/\")[1]\n","    if os.path.isfile(path) or os.path.islink(path):\n","        os.remove(path)\n","    elif os.path.isdir(path):\n","        shutil.rmtree(path)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"00620a0d99e14563a67419046e4fb8dd","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":404,"execution_start":1669824147603,"source_hash":"b94e2e9e","tags":[]},"outputs":[],"source":["import requests\n","import json\n","import math\n","from jsonmerge import merge\n","\n","class BearerAuth(requests.auth.AuthBase):\n","    def __init__(self, token):\n","        self.token = token\n","    def __call__(self, r):\n","        r.headers[\"authorization\"] = \"Bearer \" + self.token\n","        r.headers[\"user-agent\"] = github_username\n","        return r\n","\n","def get_full_name_from_gh_user(user_name):\n","    req = 'https://api.github.com/users/{un}'.format(un=user_name)\n","    res = requests.get(req, auth=BearerAuth(github_token))\n","    res_json = res.json()\n","    name = None\n","    if \"name\" in res_json:\n","        name = res_json[\"name\"]\n","    else:\n","        name = None\n","    if not isinstance(name, str):\n","        name = None\n","    return name\n","\n","def formatForkDetails(n):\n","    owner = n[\"owner\"]\n","    name = get_full_name_from_gh_user(owner[\"login\"])\n","    return json.dumps({\"owner_login\": owner[\"login\"], \"name\": name, \"created_at\": n[\"created_at\"]})\n","\n","def getForksOfRepo(repo_name):\n","    reqRepo = 'https://api.github.com/repos/{rn}'.format(rn=repo_name)\n","    resRepo = requests.get(reqRepo, auth=BearerAuth(github_token))\n","    print(resRepo.json())\n","    res_json_repo = resRepo.json()[\"forks_count\"]\n","    merged_json = None\n","    for x in range(math.ceil(res_json_repo/100)):\n","        req = 'https://api.github.com/repos/{rn}/forks?per_page=100&page={p}'.format(rn=repo_name, p=x+1)\n","        res = requests.get(req, auth=BearerAuth(github_token))\n","        res_json = res.json()\n","        if merged_json == None:\n","            merged_json = res_json\n","        else:\n","            merged_json = merged_json + res_json    \n","    formatted_res = map(formatForkDetails, merged_json)\n","    return list(formatted_res)\n","\n"," "]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"0e721005d6964df18e2a9fa7a7a16263","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1669824148010,"source_hash":"95c92b9b","tags":[]},"outputs":[],"source":["from pydriller import RepositoryMining\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","\n","def commitsFromUser(path_to_repo, usernames, from_date, to_date):\n","    res = []\n","    for commit in RepositoryMining(path_to_repo, only_authors=usernames, since=from_date, to=to_date).traverse_commits():\n","            res.append({\"lines\": commit.lines, \"date\": commit.committer_date.isoformat()})\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"20f0182003034de99023becccf73a5e2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":57658,"execution_start":1669826219742,"source_hash":"f1c3198d","tags":[]},"outputs":[],"source":["import csv\n","import json\n","import numpy as np\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","import pytz\n","\n","utc=pytz.UTC\n","\n","\n","ratelimiturl = 'https://api.github.com/users/{un}'.format(un=github_username)\n","limitremaining = requests.get(ratelimiturl, auth=BearerAuth(github_token)).headers['X-RateLimit-Remaining']\n","\n","if str(limitremaining) != '0':\n","    file = open('datareps.csv')\n","    type(file)\n","\n","\n","    csvreader = csv.reader(file)\n","\n","    header = []\n","    header = next(csvreader)\n","    header\n","    rows = []\n","    for row in csvreader:\n","            rows.append(row)\n","    rows\n","    list_of_forkCommits = []\n","\n","\n","    # Intervals due to rate limit to allow for manual start (start=inclusive, end=exclusive)\n","    intervals = [(x, x+5) for x in np.arange(20)*5]\n","    shorter_intervals = [(x+35, x+35+3) for x in np.arange(5)*3]\n","    # Change this to False after the 0th/first run\n","    overwrite = False\n","\n","    # Change this index by 1 for each run - 20 intervals with 5 each (only use 0-9), we had to change to use 5 intervals with 3 each for the last repositores due to the size of the data, adjust according to your needs\n","    interval = intervals[0]\n","    # New intervals from 35-50 [0,1,2,3,4]\n","    # New done=[0,1,2,3,4]\n","    #interval = shorter_intervals[0]\n","    MiningDate = datetime(2022, 8, 24).replace(tzinfo=utc)\n","    # Beaware of intervals include the headers multiple times (which breaks the model notebook if not removed)\n","\n","    for r in rows[interval[0]:interval[1]]:\n","        reponame = r[1].split(\"/\")[1]\n","        print(\"clone start\")\n","        cloneRepo(r[1])\n","        print(\"clone end\")\n","        forks = getForksOfRepo(r[1])\n","        for f in forks:\n","            res_json = json.loads(f)\n","            created_at_utc = datetime.fromisoformat(res_json[\"created_at\"][:-1] + '+00:00').replace(tzinfo=utc)\n","            if created_at_utc < MiningDate:\n","                res_json = json.loads(f)\n","                created_at = res_json[\"created_at\"] \n","                dt1 = datetime.fromisoformat(created_at[:-1] + '+00:00')\n","                owner_login = res_json[\"owner_login\"]\n","                name = res_json[\"name\"]\n","                dt1 = datetime.fromisoformat(created_at[:-1] + '+00:00')\n","                dt2 =   dt1 + relativedelta(months=+3)\n","                nameArr = []\n","                if \"[]\" not in owner_login:\n","                    nameArr.append(owner_login)\n","                if name != None and (not(len(name) == 0)) and \"[]\" not in name:\n","                    nameArr.append(name)\n","                commits = commitsFromUser(reponame, nameArr, dt1, dt2)\n","                forkCommit = {\n","                \"repository\" : r[1],\n","                \"owner_login\": owner_login,\n","                \"created_at\": dt1,\n","                \"end_of_range\": dt2,\n","                \"commits\": commits\n","                }\n","                list_of_forkCommits.append(forkCommit)\n","        deleteRepo(r[1])   \n","\n","\n","    import pandas as pd\n","    df = pd.DataFrame(list_of_forkCommits)\n","    if (overwrite):\n","        df.to_csv('forkCommits.csv', index=False, header=True)\n","    else:\n","        df.to_csv('forkCommits.csv', mode='a', index=False, header=True)\n","\n","    file.close()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# This can be used if writing to csv fails as it allows to dump the jupyter variable\n","import pickle\n","with open('missingdata.txt', 'wb') as f:\n","   pickle.dump(list_of_forkCommits, f)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# this allows to read the missingdata file and add it to the csv\n","import pandas as pd\n","with open('missingdata.json', 'rb') as f:\n","    data = pickle.load(f)\n","    df_missing = pd.DataFrame(data)\n","    df_missing.to_csv('forkCommits.csv', mode='a', index=False, header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"95831fc2eb5a426988ba8a7edcda785e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8293,"execution_start":1669826377846,"source_hash":"5f42c376","tags":[]},"outputs":[],"source":["import time\n","def check_rate_limit():\n","    ratelimiturl = 'https://api.github.com/users/{un}}'.format(un=github_username)\n","    limitremaining = requests.get(ratelimiturl, auth=BearerAuth(github_token)).headers['X-RateLimit-Reset']\n","    value = datetime.fromtimestamp(int(limitremaining))\n","    b = time.time() #current epoch time\n","    c = b - int(limitremaining) #returns seconds\n","    minutes = c // 60 % 60\n","    print(\"GMT timestamp until ready: \", value.strftime('%Y-%m-%d %H:%M:%S'))\n","    print(\"minutes left: \", 60-minutes)\n","check_rate_limit()"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"9b3424c5b318472eb2a41f62d0ad2611","kernelspec":{"display_name":"Python 3.7.9 64-bit ('3.7.9')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"95de1de503bc3f347dfa6159a371863c788383989342fba6ed437c78cf27f3f0"}}},"nbformat":4,"nbformat_minor":0}
